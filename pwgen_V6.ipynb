{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is aimed at modifying Yan Gobeil's code in order to generate passwords using a Recurrent Neural Network. I've provided a list of around one million passwords for the computer to train on in the hope that it can provide us with a model that somewhat accurately predicts passwords in the future.\n",
    "\n",
    "I'm new here, so if my code sucks, I'm sorry, but I'm still learning. If you have ideas on how I could improve it I'd love it if you would share them with me.\n",
    "\n",
    "Without Yan's prior work I wouldn't have been able to do this, so thank you sir!\n",
    "\n",
    "Here's a link to the man's github:\n",
    "https://github.com/yangobeil/\n",
    "\n",
    "Now without further adoo, lets get to it!\n",
    "\n",
    "BEFORE YOU START: DOWNLOAD rockyou.txt from http://downloads.skullsecurity.org/passwords/rockyou.txt.bz2\n",
    "DON'T FORGET TO UNZIP IN THE DIRECTORY OF THIS FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather and clean up the  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing that needs to be done is to import all the modules that we need for the cleaning stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section imports the password list that the network will train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# passwords = []\n",
    "# with open(\"./PWList.txt\") as f:\n",
    "#     for line in f.readlines():\n",
    "#         if len(line) <= 10:\n",
    "#             passwords.append(line.rstrip(\"\\n\"))\n",
    "\n",
    "\n",
    "# print(passwords[:10]\n",
    "\n",
    "# print(len(passwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # because my computer wasn't able to load all of the list into ram while still being able to go through the entire process I decided to randomly pick half a million passwords at random, save that as a new list, and use that for the remainder of the program.\n",
    "# passwords = random.sample(passwords, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Here the randomly picked training passwords are saved to a list to be used:\n",
    "\n",
    "# with open('halfmil.txt','w') as f:\n",
    "#   f.write('\\n'.join(passwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the password training set\n",
    "\n",
    "passwords = []\n",
    "with open(\"./halfmil.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        if len(line) <= 10:\n",
    "            passwords.append(line.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we get the model's vocabulary to get all the characters used in the training set\n",
    "\n",
    "chars_in_traininglist = []\n",
    "for pw in passwords:\n",
    "    for c in pw:\n",
    "        chars_in_traininglist.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'v', 'i', '7', 'W', 's', 'm', 'j', 'G', 'I', '\\\\', '#', 'Y', '%', 'y', '(', '2', 'C', 'k', 'e', '3', 'n', 'Z', 'T', \"'\", '-', 'q', '`', 'c', ']', 'a', 'N', 'U', 'd', 'H', 'K', 'l', 'w', 'O', 'P', 'L', 'M', '1', 'g', '~', '9', 'F', '&', 'x', '6', 'J', '{', 't', '0', 'b', '$', 'o', '!', 'p', 'h', '@', 'V', 'u', '*', '^', 'D', 'z', '?', '_', '+', 'r', 'A', 'Q', ';', 'f', 'B', '8', '.', '5', 'E', '4', '[', 'S', 'R', '|', 'X', '/'}\n86\n"
    }
   ],
   "source": [
    "print(set(chars_in_traininglist))\n",
    "print(len(set(chars_in_traininglist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('a', 217243), ('1', 205747), ('e', 183494), ('0', 147785), ('o', 144080), ('2', 140743), ('r', 134642), ('i', 131618), ('n', 126494), ('s', 117276), ('9', 114097), ('l', 103881), ('t', 103664), ('3', 96078), ('5', 88268), ('7', 86532), ('6', 84650), ('8', 83419), ('4', 82429), ('m', 80667), ('d', 77991), ('c', 70715), ('y', 70388), ('u', 66701), ('b', 64587), ('h', 64558), ('k', 63081), ('w', 58489), ('g', 55779), ('p', 51320), ('z', 47722), ('f', 45568), ('v', 43549), ('x', 36874), ('j', 31724), ('q', 12597), ('A', 10116), ('W', 9563), ('S', 8376), ('Y', 8280), ('E', 8267), ('Z', 8052), ('R', 7639), ('X', 7040), ('T', 6998), ('N', 6691), ('L', 6617), ('B', 6572), ('M', 6497), ('C', 6235), ('D', 6197), ('V', 5907), ('P', 5876), ('O', 5857), ('I', 5528), ('H', 5280), ('G', 5027), ('K', 4909), ('F', 4721), ('U', 4571), ('J', 4096), ('Q', 2655), ('_', 734), ('.', 663), ('-', 503), ('!', 333), ('*', 321), ('@', 200), ('$', 99), ('?', 87), ('#', 61), ('&', 43), (';', 26), ('%', 25), ('^', 14), (\"'\", 13), ('+', 6), ('~', 6), ('[', 5), ('\\\\', 3), ('|', 3), ('`', 2), ('(', 2), ('{', 2), ('/', 2), (']', 1)]\n"
    }
   ],
   "source": [
    "# if the need exists for a smaller vocabulary due to hardware limitations you can scan for character which only appear once or twice in the training set. This will decrease the size of each array created. This can give an idea on which characters could be excluded.\n",
    "\n",
    "freq_ch_traininglist = Counter()\n",
    "for i in chars_in_traininglist:\n",
    "    for ch in i:\n",
    "        freq_ch_traininglist[ch] +=1\n",
    "        \n",
    "print(freq_ch_traininglist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['!', '#', '$', '%', '&', \"'\", '(', '*', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '~']\n86\n"
    }
   ],
   "source": [
    "#further refinement of vocabulary\n",
    "\n",
    "extended_ascii = [chr(i) for i in range(32,256)]\n",
    "\n",
    "custom_ascii = []\n",
    "for ch in set(extended_ascii):\n",
    "    if ch in set(chars_in_traininglist):\n",
    "        custom_ascii.append(ch)\n",
    "custom_ascii = sorted(custom_ascii)\n",
    "\n",
    "print(custom_ascii)\n",
    "print(len(custom_ascii))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6\n86\n['!', '#', '$', '%', '&', \"'\", '(', '*', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '~']\n"
    }
   ],
   "source": [
    "# Here you can remove uncommon characters from your trainingset if its nessecary to shrink the array\n",
    "\n",
    "exclusion = [')','¼','é','»','ª','}']\n",
    "for i in custom_ascii:\n",
    "    if i in exclusion:\n",
    "        custom_ascii.remove(i)\n",
    "\n",
    "print(len(exclusion))\n",
    "print(len(custom_ascii))\n",
    "print(custom_ascii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "500000\n"
    }
   ],
   "source": [
    "# When making use of  the above cell also make sure that the entries in the training set which contain the excluded characters are also removed.\n",
    "\n",
    "for ch in exclusion:\n",
    "    for i in passwords:\n",
    "        if ch in i:\n",
    "            passwords.remove(i)\n",
    "\n",
    "# Print out the new length of the adjusted trainingset\n",
    "print(len(passwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "86\n"
    }
   ],
   "source": [
    "#amount of characters in trainingset: (for use later in character indexing)\n",
    "print(len(set(chars_in_traininglist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'v', 'i', '7', 'W', 's', 'm', 'j', 'G', 'I', '\\\\', '#', 'Y', '%', 'y', '(', '2', 'C', 'k', 'e', '3', 'n', 'Z', 'T', \"'\", '-', 'q', '`', 'c', ']', 'a', 'N', 'U', 'd', 'H', 'K', 'l', 'w', 'O', 'P', 'L', 'M', '1', 'g', '~', '9', 'F', '&', 'x', '6', 'J', '{', 't', '0', 'b', '$', 'o', '!', 'p', 'h', '@', 'V', 'u', '*', '^', 'D', 'z', '?', '_', '+', 'r', 'A', 'Q', ';', 'f', 'B', '8', '.', '5', 'E', '4', '[', 'S', 'R', '|', 'X', '/'}\n86\n"
    }
   ],
   "source": [
    "chars_in_traininglist = []\n",
    "for pw in passwords:\n",
    "    for c in pw:\n",
    "        chars_in_traininglist.append(c)\n",
    "print(set(chars_in_traininglist))\n",
    "print(len(set(chars_in_traininglist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9\n"
    }
   ],
   "source": [
    "# Length of the longest password in the training set\n",
    "print (len(max(passwords, key=len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should add a ',' at the end of each password. This will be useful to tell the RNN that the password is at its end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['arnasin,',\n 'peregrina,',\n '777a77,',\n 'Dfktynbyf,',\n '50055005,',\n 'rachelm,',\n 'zcfv131,',\n 'since1989,',\n '03011970,',\n '3335577,']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "passwords = list(map(lambda s: s + ',', passwords ))\n",
    "passwords [:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data cleaned up it's time to transform it into a form that the neural network will understand. The details of the model will be explained later but all we need to know is that we will input characters into the network instead of words. Each of these characters will then be converted to numbers and the conversion is done using the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'!': 0, '#': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, '(': 6, '*': 7, '+': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ';': 22, '?': 23, '@': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, '\\\\': 52, ']': 53, '^': 54, '_': 55, '`': 56, 'a': 57, 'b': 58, 'c': 59, 'd': 60, 'e': 61, 'f': 62, 'g': 63, 'h': 64, 'i': 65, 'j': 66, 'k': 67, 'l': 68, 'm': 69, 'n': 70, 'o': 71, 'p': 72, 'q': 73, 'r': 74, 's': 75, 't': 76, 'u': 77, 'v': 78, 'w': 79, 'x': 80, 'y': 81, 'z': 82, '{': 83, '|': 84, '~': 85, ',': 86}\n{0: '!', 1: '#', 2: '$', 3: '%', 4: '&', 5: \"'\", 6: '(', 7: '*', 8: '+', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ';', 23: '?', 24: '@', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: '[', 52: '\\\\', 53: ']', 54: '^', 55: '_', 56: '`', 57: 'a', 58: 'b', 59: 'c', 60: 'd', 61: 'e', 62: 'f', 63: 'g', 64: 'h', 65: 'i', 66: 'j', 67: 'k', 68: 'l', 69: 'm', 70: 'n', 71: 'o', 72: 'p', 73: 'q', 74: 'r', 75: 's', 76: 't', 77: 'u', 78: 'v', 79: 'w', 80: 'x', 81: 'y', 82: 'z', 83: '{', 84: '|', 85: '~', 86: ','}\n"
    }
   ],
   "source": [
    "# Convert from character to index\n",
    "\n",
    "# Important: if you adjusted the trainingset to exclude uncommon characters be sure to adjust the char_to_index and index_to_char to the common \n",
    "\n",
    "char_to_index = dict((custom_ascii[i] , i) for i in range(0,len(set(chars_in_traininglist))))\n",
    "char_to_index[','] = 86\n",
    "print(char_to_index)\n",
    "\n",
    "index_to_char = dict((i, custom_ascii[i]) for i in range(0,len(set(chars_in_traininglist))))\n",
    "index_to_char[86] = ',' \n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a few constants that will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10\n500000\n87\n"
    }
   ],
   "source": [
    "# maximum number of characters in passwords\n",
    "# this will be the number of time steps in the RNN\n",
    "max_char= len(max(passwords, key=len))\n",
    "print(max_char)\n",
    "\n",
    "# number of elements in the list of passwords, this is the number of training examples\n",
    "m = len(passwords)\n",
    "print(m)\n",
    "\n",
    "# number of potential characters, this is the length of the input of each of the RNN units\n",
    "char_dim = len(char_to_index)\n",
    "print(char_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we convert the list of passwords into a training dataset. The input X of the network is an array of size (m, max_char, char_dim). It contains a matrix for each of the m names. Each matrix contains a row for each character in the name. (Note that there are always the same number of rows and if the name doesn't have enough characters to fill the whole matrix the remaining rows contain nothing.) Each of these rows represents one character and it is encoded as a one-hot vector. This means that it is a vector of zeros with a one only in the entry that corresponds to the character that is present.\n",
    "\n",
    "The output Y is the same as the input but translated by one unit. This means that the ith character in Y is the (i+1)th one in the actual password. This means that the network predicts the character that follows a given character in a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((m, max_char, char_dim))\n",
    "Y = np.zeros((m, max_char, char_dim))\n",
    "\n",
    "for i in range(m):\n",
    "    pw = list(passwords[i])\n",
    "    for j in range(len(pw)):\n",
    "        X[i, j, char_to_index[pw[j]]] = 1\n",
    "        if j < len(pw)-1:\n",
    "            Y[i, j, char_to_index[pw[j+1]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we will use is a many-to-many recurrent neural network. This is a network that contains a given number of 'time' steps that each act with the same weights on the individual inputs and are all connected. Each time step takes in one input (in this case one character), and outputs a one-hot vector that represents the probabilities for the input of the next time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of interest here we only consider one layer of recurrence, which we take to be LSTM with 128 units. We return the output of this layer and use it into a fully connected dense layer that converts the result of the LSTM layer into a vector of size char_dim using a softmax activation. We use categorical cross entropy as a cost function because of the softmax result and use Adam optimization. There is not really any useful metric to judge if the model does good (in the future I might impliment a function where it queries the HIBP database to see if the password has occured in the wild, and if so to see how often, this might be included as feedback to tell the machine whether its doing good or not.) for now though we will mostly just look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_char, char_dim), return_sequences=True))\n",
    "model.add(Dense(char_dim, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this model will be trained we will use it to create new passwords. This is achieved using the following function. The idea is to input empty characters to the trained network and use the output of the first time step as a probability distribution for the first letter of the name. We then use this distribution to decide randomly the first character, record it and update the input to pass this character as an input for the second time step. This is continued for the following time steps to create a password.\n",
    "\n",
    "This is where using a ',' at the end of each name becomes important, because we stop the procedure once we get a ',' as an output, meaning that the generated password is done. Also if we reach the length of the largest name in the training set we put a ',' and end the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pw(model):\n",
    "    pw = []\n",
    "    x = np.zeros((1, max_char, char_dim))\n",
    "    end = False\n",
    "    i = 0\n",
    "    \n",
    "    while end==False:\n",
    "        probs = list(model.predict(x)[0,i])\n",
    "        probs = probs / np.sum(probs)\n",
    "        index = np.random.choice(range(char_dim), p=probs)\n",
    "        if i == max_char-2:\n",
    "            character = ','\n",
    "            end = True\n",
    "        else:\n",
    "            character = index_to_char[index]\n",
    "        pw.append(character)\n",
    "        x[0, i+1, index] = 1\n",
    "        i += 1\n",
    "        if character == ',':\n",
    "            end = True\n",
    "    \n",
    "    return ''.join(pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use this function during the training to monitor how the generated passwords get better. To this end we create a function that will be given to the model when we fit it. We basically run the previous function a few times every epoch and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pw_loop(epoch, _):\n",
    "    if epoch % 1 == 0:\n",
    "        print('Passwords generated after epoch %d:' % epoch)\n",
    "\n",
    "        for i in range(3):\n",
    "            make_pw(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converts the function to be able to use it in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_generator = LambdaCallback(on_epoch_end = generate_pw_loop)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/checkpoint-{epoch:03d}.h5\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# This saves the model each epoch\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=False,\n",
    "    verbose=1,\n",
    "    period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(model, nr_of_samples):\n",
    "    sample_list = []\n",
    "\n",
    "    for i in range(nr_of_samples):\n",
    "        pw = make_pw(model)\n",
    "        new_pw = pw[:-1]\n",
    "        sample_list.append(new_pw)\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_against_rockyou(model, times, nr_of_samples):\n",
    "    # open rockyou list\n",
    "    rockyou_list = []\n",
    "    with open('./rockyou.txt',mode='r',encoding='latin-1') as f:\n",
    "        for line in f.readlines():\n",
    "            rockyou_list.append(line.rstrip(\"\\n\"))\n",
    "\n",
    "\n",
    "    model_test_results = []\n",
    "    for x in tqdm(range(times)):\n",
    "        #sample 1000\n",
    "        sample_list = sample(model, nr_of_samples)\n",
    "\n",
    "        # test_against_rockyou\n",
    "        rockyou_hits = 0\n",
    "        for sample_item in sample_list:\n",
    "            for rockyou_pw in rockyou_list:\n",
    "                if sample_item == rockyou_pw:\n",
    "                    rockyou_hits += 1\n",
    "\n",
    "        model_test_results.append(rockyou_hits)\n",
    "    return model_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be possible to make the network better by changing the hyperparameters (number of units in LSTM layer, parameters of Adam optimization, adding extra LSTM layer) but I am not currently able to do that. One thing to notice is that there are some cases where the model seems to have overfitted and we recover known names or names very close to known ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# To load a previously saved model\n",
    "# new_model = load_model(\"./checkpoints/checkpoint-010.h5\")\n",
    "# print(\"Model summary: \\n\\n\")\n",
    "\n",
    "# Start Training from scratch, or continue_training if you've got a checkpoint loaded\n",
    "# new_model.fit(X, Y, batch_size=64, epochs=1, callbacks=[pw_generator, model_checkpoint_callback])\n",
    "\n",
    "# Summarize model\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use the final trained model to generate as many paswords as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['eastysa', '285011', 'roggand', 'ifee', 'vbnrzt']\n"
    }
   ],
   "source": [
    "# prints a few examples for refference\n",
    "\n",
    "# chose which model you want to load\n",
    "new_model = load_model(\"./checkpoints/checkpoint-040.h5\")\n",
    "\n",
    "# print 5 new passwords:\n",
    "\n",
    "run = sample(new_model, 5)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/2 [00:00<?, ?it/s]i\n 50%|█████     | 1/2 [02:54<02:54, 174.68s/it]i\n100%|██████████| 2/2 [05:47<00:00, 173.79s/it]\n<class 'list'>\nAmount of generated password which match rockyou.txt = 93\nTotal = 93\nAmount of generated password which match rockyou.txt = 81\nTotal = 174\nAverage percentage of generated password which match rockyou.txt = 17.4%\n"
    }
   ],
   "source": [
    "# Testing generated passwords against the rockyou.txt list\n",
    "\n",
    "runs = 2\n",
    "new_model = load_model(\"./checkpoints/checkpoint-040.h5\")\n",
    "nr_of_samples = 500\n",
    "\n",
    "test_result = test_against_rockyou(new_model, runs , nr_of_samples)\n",
    "print(type(test_result))\n",
    "\n",
    "total = 0\n",
    "for i in test_result:\n",
    "    print(\"Amount of generated password which match rockyou.txt = \" + str(i))\n",
    "    total += i\n",
    "    print(\"Total = \" + str(total))\n",
    "\n",
    "print(\"Average percentage of generated password which match rockyou.txt = \" + str(((total/runs) / nr_of_samples) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}